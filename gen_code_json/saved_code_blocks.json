[
  {
    "language": "python",
    "code": "import pandas as pd\nimport numpy as np\nimport os\nimport pytz\nfrom datetime import timedelta\n\nclass MetricAnomalyDetector:\n    \"\"\"\n    A class to detect metric anomalies based on a defined workflow.\n    \"\"\"\n    def __init__(self, start_str, end_str, data_path_template):\n        self.tz = pytz.timezone('Asia/Shanghai')\n        self.diagnosis_start = self.tz.localize(pd.to_datetime(start_str))\n        self.diagnosis_end = self.tz.localize(pd.to_datetime(end_str))\n        \n        # Extend window by 15 mins before and after for context\n        self.extended_start = self.diagnosis_start - timedelta(minutes=15)\n        self.extended_end = self.diagnosis_end + timedelta(minutes=15)\n        \n        self.diagnosis_start_ts = int(self.diagnosis_start.timestamp())\n        self.diagnosis_end_ts = int(self.diagnosis_end.timestamp())\n        self.extended_start_ts = int(self.extended_start.timestamp())\n        self.extended_end_ts = int(self.extended_end.timestamp())\n        \n        self.data_path = data_path_template.format(date=self.diagnosis_start.strftime('%Y_%m_%d'))\n        \n        self.kpis_to_check = [\n            'OSLinux-CPU_CPU_CPUCpuUtil', 'OSLinux-CPU_CPU_CPUUserTime',\n            'OSLinux-OSLinux_MEMORY_MEMORY_NoCacheMemPerc', 'OSLinux-OSLinux_MEMORY_MEMORY_MEMUsedMemPerc', 'OSLinux-OSLinux_MEMORY_MEMORY_MEMFreeMem',\n            'OSLinux-OSLinux_NETWORK_NETWORK_TCP-FIN-WAIT', 'OSLinux-OSLinux_NETWORK_NETWORK_TotalTcpConnNum',\n            'OSLinux-OSLinux_LOCALDISK_LOCALDISK-sdb_DSKReadWrite',\n            'OSLinux-OSLinux_FILESYSTEM_-tomcat_FSCapacity', 'OSLinux-OSLinux_FILESYSTEM_-apache_FSCapacity',\n            'JVM-Operating System_7779_JVM_JVM_CPULoad', 'JVM-Operating System_7778_JVM_JVM_CPULoad',\n            'JVM-Memory_7778_JVM_Memory_NoHeapMemoryUsed', 'JVM-Memory_7779_JVM_Memory_NoHeapMemoryUsed'\n        ]\n        \n        # Detection parameters\n        self.STABLE_PHASE_LEN = 5\n        self.ANOMALY_PHASE_MAX_LEN = 8\n        self.RECOVERY_PHASE_LEN = 5\n        self.STABILITY_STD_THRESHOLD = 10.0\n        self.SLOPE_THRESHOLD = 15.0\n        self.NOISE_REDUCTION_W = 0.6\n        self.CLUSTER_TIME_GAP_MINS = 3\n        self.MAX_CLUSTERS = 3\n        self.FINAL_FILTER_X = 0.2\n\n    def load_data(self):\n        \"\"\"Loads and filters data for the extended time window.\"\"\"\n        if not os.path.exists(self.data_path):\n            return None\n        df = pd.read_csv(self.data_path)\n        df_filtered = df[(df['timestamp'] >= self.extended_start_ts) & (df['timestamp'] <= self.extended_end_ts)].copy()\n        df_filtered['datetime'] = pd.to_datetime(df_filtered['timestamp'], unit='s', utc=True).dt.tz_convert(self.tz)\n        return df_filtered\n\n    def normalize_metric(self, df_metric):\n        \"\"\"Normalizes metric values to a 0-100 scale.\"\"\"\n        min_val = df_metric['value'].min()\n        max_val = df_metric['value'].max()\n        if max_val == min_val:\n            df_metric['norm_value'] = 0.0\n        else:\n            df_metric['norm_value'] = 100 * (df_metric['value'] - min_val) / (max_val - min_val)\n        return df_metric\n\n    def find_anomalies_in_series(self, series, is_spike):\n        \"\"\"\n        Finds anomaly events (stable-anomaly-recovery pattern) in a single time series.\n        \"\"\"\n        events = []\n        search_series = series[(series.index >= self.diagnosis_start) & (series.index <= self.diagnosis_end)]\n\n        for i in range(len(search_series)):\n            for anomaly_len in range(1, self.ANOMALY_PHASE_MAX_LEN + 1):\n                anomaly_start_idx_abs = series.index.get_loc(search_series.index[i])\n                \n                if anomaly_start_idx_abs < self.STABLE_PHASE_LEN or \\\n                   anomaly_start_idx_abs + anomaly_len + self.RECOVERY_PHASE_LEN > len(series):\n                    continue\n                \n                anomaly_end_time = series.index[anomaly_start_idx_abs + anomaly_len - 1]\n                if not (self.diagnosis_start <= anomaly_end_time <= self.diagnosis_end):\n                    continue\n\n                stable_slice = series.iloc[anomaly_start_idx_abs - self.STABLE_PHASE_LEN : anomaly_start_idx_abs]\n                anomaly_slice = series.iloc[anomaly_start_idx_abs : anomaly_start_idx_abs + anomaly_len]\n                recovery_slice = series.iloc[anomaly_start_idx_abs + anomaly_len : anomaly_start_idx_abs + anomaly_len + self.RECOVERY_PHASE_LEN]\n\n                if len(stable_slice) < self.STABLE_PHASE_LEN or len(recovery_slice) < self.RECOVERY_PHASE_LEN:\n                    continue\n\n                if stable_slice.std() < self.STABILITY_STD_THRESHOLD and recovery_slice.std() < self.STABILITY_STD_THRESHOLD:\n                    stable_avg = stable_slice.mean()\n                    \n                    if is_spike:\n                        if anomaly_slice.iloc[0] - stable_slice.iloc[-1] > self.SLOPE_THRESHOLD:\n                            delta = anomaly_slice.max() - stable_avg\n                            if delta > 0:\n                                events.append({'start_time': anomaly_slice.index[0], 'peak_time': anomaly_slice.idxmax(), 'delta': delta})\n                    else: # Drop\n                        if stable_slice.iloc[-1] - anomaly_slice.iloc[0] > self.SLOPE_THRESHOLD:\n                            delta = stable_avg - anomaly_slice.min()\n                            if delta > 0:\n                                events.append({'start_time': anomaly_slice.index[0], 'peak_time': anomaly_slice.idxmin(), 'delta': delta})\n        \n        if not events: return []\n        return pd.DataFrame(events).sort_values('delta', ascending=False).drop_duplicates('start_time').to_dict('records')\n\n    def find_events_for_metric(self, df_metric, kpi_name):\n        \"\"\"\n        Orchestrates the anomaly detection process for a single metric.\n        \"\"\"\n        df_metric = self.normalize_metric(df_metric.copy())\n        pivot_df = df_metric.pivot_table(index='datetime', columns='cmdb_id', values='norm_value')\n        diag_window_df = pivot_df.loc[self.diagnosis_start:self.diagnosis_end]\n        if diag_window_df.empty: return []\n            \n        sorted_components = diag_window_df.max().sort_values(ascending=False).index\n        \n        metric_anomalies = []\n        for cmdb_id in sorted_components:\n            component_series = pivot_df[cmdb_id].dropna()\n            if len(component_series) < (self.STABLE_PHASE_LEN + 1 + self.RECOVERY_PHASE_LEN): continue\n            \n            all_events = self.find_anomalies_in_series(component_series, is_spike=True) + \\\n                         self.find_anomalies_in_series(component_series, is_spike=False)\n            \n            for event in sorted(all_events, key=lambda x: x['delta'], reverse=True)[:2]:\n                metric_anomalies.append({'cmdb_id': cmdb_id, 'kpi_name': kpi_name, **event})\n\n        if not metric_anomalies: return []\n\n        max_delta = max(e['delta'] for e in metric_anomalies)\n        denoised_anomalies = [e for e in metric_anomalies if e['delta'] >= self.NOISE_REDUCTION_W * max_delta]\n        if not denoised_anomalies: return []\n            \n        denoised_anomalies.sort(key=lambda x: x['start_time'])\n        \n        clusters = []\n        if denoised_anomalies:\n            current_cluster = [denoised_anomalies[0]]\n            clusters.append(current_cluster)\n            for i in range(1, len(denoised_anomalies)):\n                time_diff = (denoised_anomalies[i]['start_time'] - current_cluster[-1]['start_time']).total_seconds()\n                if time_diff <= self.CLUSTER_TIME_GAP_MINS * 60:\n                    current_cluster.append(denoised_anomalies[i])\n                else:\n                    current_cluster = [denoised_anomalies[i]]\n                    clusters.append(current_cluster)\n        \n        if len(clusters) > self.MAX_CLUSTERS: return []\n\n        denoised_anomalies.sort(key=lambda x: x['delta'], reverse=True)\n        top_events = denoised_anomalies[:2]\n        \n        def get_event_id(event):\n            return (event['cmdb_id'], event['kpi_name'], event['start_time'])\n\n        top_event_ids = {get_event_id(e) for e in top_events}\n        \n        final_anomalies = []\n        added_event_ids = set()\n\n        for cluster in clusters:\n            is_root_cause_cluster = False\n            for event_in_cluster in cluster:\n                if get_event_id(event_in_cluster) in top_event_ids:\n                    is_root_cause_cluster = True\n                    break\n            \n            if is_root_cause_cluster:\n                for event_in_cluster in cluster:\n                    event_id = get_event_id(event_in_cluster)\n                    if event_id not in added_event_ids:\n                        final_anomalies.append(event_in_cluster)\n                        added_event_ids.add(event_id)\n        \n        return final_anomalies\n\n    def run(self):\n        \"\"\"\n        Main execution function.\n        \"\"\"\n        full_df = self.load_data()\n        if full_df is None or full_df.empty:\n            print(\"anomaly_events = []\")\n            pd.DataFrame(columns=[\"data_source\", \"timestamp\", \"cmdb_id\", \"description\"]).to_csv(\"anomaly_events.csv\", index=False)\n            return\n\n        all_anomalies = []\n        for kpi in self.kpis_to_check:\n            kpi_df = full_df[full_df['kpi_name'] == kpi]\n            if not kpi_df.empty:\n                all_anomalies.extend(self.find_events_for_metric(kpi_df, kpi))\n\n        if not all_anomalies:\n            print(\"anomaly_events = []\")\n            pd.DataFrame(columns=[\"data_source\", \"timestamp\", \"cmdb_id\", \"description\"]).to_csv(\"anomaly_events.csv\", index=False)\n            return\n\n        overall_max_delta = max(e['delta'] for e in all_anomalies)\n        final_anomalies = [e for e in all_anomalies if e['delta'] >= self.FINAL_FILTER_X * overall_max_delta]\n        \n        output_events = [{\n            \"data_source\": \"Metric\",\n            \"timestamp\": event['peak_time'].strftime('%Y-%m-%d %H:%M:%S'),\n            \"cmdb_id\": event['cmdb_id'],\n            \"description\": f\"Metric '{event['kpi_name']}' is abnormally high/low, delta={event['delta']:.2f}\"\n        } for event in final_anomalies]\n            \n        output_events.sort(key=lambda x: float(x['description'].split('=')[-1]), reverse=True)\n\n        print(\"anomaly_events = [\")\n        for i, event in enumerate(output_events):\n            print(f\"    {event}\" + (\",\" if i < len(output_events) - 1 else \"\"))\n        print(\"]\")\n        \n        if output_events:\n            pd.DataFrame(output_events).to_csv(\"anomaly_events.csv\", index=False)\n        else:\n            pd.DataFrame(columns=[\"data_source\", \"timestamp\", \"cmdb_id\", \"description\"]).to_csv(\"anomaly_events.csv\", index=False)\n\n\ndef main():\n    start_time_str = \"2021-03-04 18:00:00\"\n    end_time_str = \"2021-03-04 18:30:00\"\n    data_path_template = \"./dataset/Bank/telemetry/{date}/metric/metric_container.csv\"\n    \n    detector = MetricAnomalyDetector(start_time_str, end_time_str, data_path_template)\n    detector.run()\n\nif __name__ == \"__main__\":\n    main()\n",
    "timestamp": "2025-07-25 23:48:16"
  },
  {
    "language": "python",
    "code": "import pandas as pd\nimport numpy as np\nimport pytz\nfrom datetime import datetime, timedelta\nimport os\n\ndef find_anomalies():\n    \"\"\"\n    Main function to orchestrate the anomaly detection process.\n    \"\"\"\n    # 1. Configuration\n    TARGET_KPIS = [\n        'OSLinux-CPU_CPU_CPUCpuUtil', 'OSLinux-CPU_CPU_CPUUserTime',\n        'OSLinux-OSLinux_MEMORY_MEMORY_NoCacheMemPerc', 'OSLinux-OSLinux_MEMORY_MEMORY_MEMUsedMemPerc', 'OSLinux-OSLinux_MEMORY_MEMORY_MEMFreeMem',\n        'OSLinux-OSLinux_NETWORK_NETWORK_TCP-FIN-WAIT', 'OSLinux-OSLinux_NETWORK_NETWORK_TotalTcpConnNum',\n        'OSLinux-OSLinux_LOCALDISK_LOCALDISK-sdb_DSKReadWrite',\n        'OSLinux-OSLinux_FILESYSTEM_-tomcat_FSCapacity', 'OSLinux-OSLinux_FILESYSTEM_-apache_FSCapacity',\n        'JVM-Operating System_7779_JVM_JVM_CPULoad', 'JVM-Operating System_7778_JVM_JVM_CPULoad',\n        'JVM-Memory_7778_JVM_Memory_NoHeapMemoryUsed', 'JVM-Memory_7779_JVM_Memory_NoHeapMemoryUsed'\n    ]\n    DROP_KPIS = ['OSLinux-OSLinux_MEMORY_MEMORY_MEMFreeMem']\n    \n    # Time window configuration\n    tz = pytz.timezone('Asia/Shanghai')\n    start_time_str = \"2021-03-04 18:00:00\"\n    end_time_str = \"2021-03-04 18:30:00\"\n    \n    target_start_dt = tz.localize(datetime.strptime(start_time_str, \"%Y-%m-%d %H:%M:%S\"))\n    target_end_dt = tz.localize(datetime.strptime(end_time_str, \"%Y-%m-%d %H:%M:%S\"))\n    \n    extended_start_dt = target_start_dt - timedelta(minutes=15)\n    extended_end_dt = target_end_dt + timedelta(minutes=15)\n\n    # 2. Data Loading and Preparation\n    try:\n        df = load_and_prepare_data(extended_start_dt, extended_end_dt, TARGET_KPIS)\n        if df.empty:\n            print(\"anomaly_events = []\")\n            return\n    except FileNotFoundError:\n        print(f\"Error: Data file not found for date {extended_start_dt.strftime('%Y_%m_%d')}.\")\n        print(\"anomaly_events = []\")\n        return\n\n    # 3. Normalization\n    df_normalized = normalize_metrics(df)\n\n    # 4. Anomaly Detection Loop\n    all_kpi_events = []\n    for kpi_name, group in df_normalized.groupby('kpi_name'):\n        pivoted_df = group.pivot_table(index='datetime', columns='cmdb_id', values='normalized_value')\n        pivoted_df = pivoted_df.resample('1min').mean().ffill().bfill()\n\n        if pivoted_df.empty:\n            continue\n\n        target_window_df = pivoted_df.loc[target_start_dt:target_end_dt]\n        if target_window_df.empty:\n            continue\n            \n        peak_values = target_window_df.max().sort_values(ascending=False)\n        sorted_components = peak_values.index\n\n        kpi_events = []\n        is_drop = kpi_name in DROP_KPIS\n        \n        for cmdb_id in sorted_components:\n            series = pivoted_df[cmdb_id].dropna()\n            if series.empty:\n                continue\n            \n            events = find_events_in_series(series, target_start_dt, target_end_dt, is_drop)\n            for event in events:\n                event['kpi_name'] = kpi_name\n            kpi_events.extend(events)\n        \n        # 5. Per-KPI Filtering\n        denoised_events = denoise_kpi_events(kpi_events, w=0.6)\n        clustered_events = cluster_and_filter_events(denoised_events, k_minutes=3, max_clusters=3)\n        all_kpi_events.extend(clustered_events)\n\n    # 6. Global Filtering and Final Output\n    final_events = final_denoise(all_kpi_events, x=0.2)\n    output_results(final_events)\n\ndef load_and_prepare_data(start_dt, end_dt, kpis):\n    \"\"\"Loads and filters metric data.\"\"\"\n    date_str = start_dt.strftime('%Y_%m_%d')\n    file_path = f'./dataset/Bank/telemetry/{date_str}/metric/metric_container.csv'\n    \n    df = pd.read_csv(file_path)\n    df = df[df['kpi_name'].isin(kpis)]\n    \n    df['datetime'] = pd.to_datetime(df['timestamp'], unit='s', utc=True).dt.tz_convert('Asia/Shanghai')\n    df = df[(df['datetime'] >= start_dt) & (df['datetime'] <= end_dt)]\n    \n    return df\n\ndef normalize_metrics(df):\n    \"\"\"Normalizes metric values to a 0-100 scale.\"\"\"\n    df_out = df.copy()\n    df_out['normalized_value'] = 0.0\n    for kpi_name, group in df.groupby('kpi_name'):\n        min_val, max_val = group['value'].min(), group['value'].max()\n        if max_val == min_val:\n            df_out.loc[group.index, 'normalized_value'] = 0.0\n        else:\n            df_out.loc[group.index, 'normalized_value'] = 100 * (group['value'] - min_val) / (max_val - min_val)\n    return df_out\n\ndef find_events_in_series(series, target_start, target_end, is_drop, STABLE_WINDOW=5, ANOMALY_WINDOW_MAX=8, STABLE_RANGE_THRESH=15, SPIKE_DELTA_THRESH=25):\n    \"\"\"Detects anomaly events in a single time series.\"\"\"\n    events = []\n    series_len = len(series)\n    \n    try:\n        target_indices = series.loc[target_start:target_end].index\n        if len(target_indices) == 0: return []\n        start_idx_loc = series.index.get_loc(target_indices[0])\n        end_idx_loc = series.index.get_loc(target_indices[-1])\n    except KeyError:\n        return []\n\n    covered_indices = set()\n    for i in range(start_idx_loc, end_idx_loc + 1):\n        if i in covered_indices: continue\n\n        stable_start_loc = i - STABLE_WINDOW\n        if stable_start_loc < 0: continue\n        \n        stable_series = series.iloc[stable_start_loc:i]\n        if stable_series.isnull().any() or len(stable_series) < STABLE_WINDOW: continue\n\n        if (stable_series.max() - stable_series.min()) >= STABLE_RANGE_THRESH: continue\n        \n        avg_stable = stable_series.mean()\n        is_significant_change = (series.iloc[i] > avg_stable + SPIKE_DELTA_THRESH) if not is_drop else (series.iloc[i] < avg_stable - SPIKE_DELTA_THRESH)\n        \n        if not is_significant_change: continue\n\n        anomaly_end_loc = i\n        for j in range(i + 1, min(i + ANOMALY_WINDOW_MAX, series_len)):\n            if series.index[j] > target_end: break\n            val_j = series.iloc[j]\n            is_still_anomalous = (val_j > avg_stable + SPIKE_DELTA_THRESH) if not is_drop else (val_j < avg_stable - SPIKE_DELTA_THRESH)\n            if is_still_anomalous:\n                anomaly_end_loc = j\n            else:\n                break\n        \n        recovery_start_loc = anomaly_end_loc + 1\n        recovery_end_loc = recovery_start_loc + STABLE_WINDOW\n        if recovery_end_loc > series_len: continue\n            \n        recovery_series = series.iloc[recovery_start_loc:recovery_end_loc]\n        if recovery_series.isnull().any() or len(recovery_series) < STABLE_WINDOW: continue\n        if (recovery_series.max() - recovery_series.min()) >= STABLE_RANGE_THRESH: continue\n\n        anomaly_series = series.iloc[i : anomaly_end_loc + 1]\n        peak_or_valley = anomaly_series.max() if not is_drop else anomaly_series.min()\n        delta = abs(peak_or_valley - avg_stable)\n        \n        events.append({'timestamp': series.index[i], 'cmdb_id': series.name, 'delta': delta})\n        for k in range(i, anomaly_end_loc + 1): covered_indices.add(k)\n\n    return sorted(events, key=lambda x: x['delta'], reverse=True)[:2]\n\ndef denoise_kpi_events(events, w):\n    \"\"\"Filters events for a single KPI based on delta.\"\"\"\n    if not events: return []\n    max_delta = max(event['delta'] for event in events)\n    return [event for event in events if event['delta'] >= w * max_delta]\n\ndef cluster_and_filter_events(events, k_minutes, max_clusters):\n    \"\"\"Clusters events by time and filters if too many clusters.\"\"\"\n    if not events: return []\n    \n    events.sort(key=lambda x: x['timestamp'])\n    clusters = []\n    if events:\n        current_cluster = [events[0]]\n        for i in range(1, len(events)):\n            time_diff = (events[i]['timestamp'] - current_cluster[-1]['timestamp']).total_seconds() / 60\n            if time_diff <= k_minutes:\n                current_cluster.append(events[i])\n            else:\n                clusters.append(current_cluster)\n                current_cluster = [events[i]]\n        clusters.append(current_cluster)\n\n    if len(clusters) > max_clusters:\n        return []\n\n    # Retain events from the top two clusters by max delta\n    if not clusters: return []\n    clusters.sort(key=lambda c: max(e['delta'] for e in c), reverse=True)\n    \n    final_events = []\n    for cluster in clusters[:2]:\n        final_events.extend(cluster)\n        \n    return final_events\n\ndef final_denoise(events, x):\n    \"\"\"Filters all events globally based on delta.\"\"\"\n    if not events: return []\n    global_max_delta = max(event['delta'] for event in events)\n    return [event for event in events if event['delta'] >= x * global_max_delta]\n\ndef output_results(events):\n    \"\"\"Formats and prints the final anomaly events.\"\"\"\n    if not events:\n        print(\"anomaly_events = []\")\n        # Create empty csv\n        pd.DataFrame(columns=['data_source', 'timestamp', 'cmdb_id', 'description']).to_csv('anomaly_events.csv', index=False)\n        return\n\n    output_list = []\n    for event in events:\n        description = f\"Metric '{event['kpi_name']}' on component '{event['cmdb_id']}' is abnormal, delta={event['delta']:.2f}\"\n        output_list.append({\n            \"data_source\": \"Metric\",\n            \"timestamp\": event['timestamp'].strftime('%Y-%m-%d %H:%M:%S'),\n            \"cmdb_id\": event['cmdb_id'],\n            \"description\": description\n        })\n    \n    # Sort by timestamp for final presentation\n    output_list.sort(key=lambda x: x['timestamp'])\n    \n    print(\"anomaly_events = [\")\n    for i, item in enumerate(output_list):\n        print(f\"    {item}\" + (\",\" if i < len(output_list) - 1 else \"\"))\n    print(\"]\")\n\n    # Save to CSV\n    df_out = pd.DataFrame(output_list)\n    df_out.to_csv('anomaly_events.csv', index=False)\n\nif __name__ == '__main__':\n    find_anomalies()\n",
    "timestamp": "2025-07-25 23:51:14"
  },
  {
    "language": "python",
    "code": "import pandas as pd\nimport pytz\nfrom datetime import datetime, timedelta\nimport os\nimport csv\n\ndef detect_trace_anomalies():\n    \"\"\"\n    Detects trace-based anomalies by identifying significant drops in span counts.\n    \"\"\"\n    # --- Configuration ---\n    DATA_DIR = './dataset/Bank/telemetry/'\n    DATE = '2021_03_04'\n    START_TIME_STR = f'{DATE.replace(\"_\", \"-\")} 18:00:00'\n    END_TIME_STR = f'{DATE.replace(\"_\", \"-\")} 18:30:00'\n    TIMEZONE = pytz.timezone('Asia/Shanghai')\n\n    # Anomaly detection parameters - INCREASED THRESHOLDS to reduce noise\n    EXTEND_MINUTES = 5\n    COUNT_DROP_AGG_FREQ_INITIAL = \"1min\"\n    COUNT_DROP_WINDOW_MINUTES = 3\n    # Increased from 0.50 to 0.75 to focus on severe drops\n    COUNT_DROP_THRESHOLD_RATIO = 0.75\n    # Increased from 1 to 2 to require multiple components to fail simultaneously\n    COUNT_DROP_MIN_COMPONENTS = 2\n    TARGET_COMPONENTS = ['Tomcat01', 'Tomcat02', 'Tomcat03', 'Tomcat04', 'IG01', 'IG02', 'MG01', 'MG02']\n\n    # --- Main Logic ---\n\n    # 1. Time setup\n    try:\n        start_time = TIMEZONE.localize(datetime.strptime(START_TIME_STR, '%Y-%m-%d %H:%M:%S'))\n        end_time = TIMEZONE.localize(datetime.strptime(END_TIME_STR, '%Y-%m-%d %H:%M:%S'))\n        extended_start_time = start_time - timedelta(minutes=EXTEND_MINUTES)\n    except Exception as e:\n        print(f\"Error parsing time: {e}\")\n        return\n\n    # 2. Load and preprocess data\n    trace_file_path = os.path.join(DATA_DIR, DATE, 'trace', 'trace_span.csv')\n    if not os.path.exists(trace_file_path):\n        print(f\"Trace file not found: {trace_file_path}\")\n        print(\"anomaly_events = []\")\n        return\n\n    df_trace = pd.read_csv(trace_file_path)\n\n    # Filter for target components\n    df_trace = df_trace[df_trace['cmdb_id'].isin(TARGET_COMPONENTS)]\n\n    # Convert timestamp and filter by extended time window\n    df_trace['datetime'] = pd.to_datetime(df_trace['timestamp'], unit='ms', utc=True).dt.tz_convert(TIMEZONE)\n    df_trace = df_trace[(df_trace['datetime'] >= extended_start_time) & (df_trace['datetime'] < end_time)]\n\n    if df_trace.empty:\n        print(\"anomaly_events = []\")\n        with open(\"anomaly_events.csv\", 'w', newline='') as f:\n            writer = csv.writer(f)\n            writer.writerow(['data_source', 'timestamp', 'cmdb_id', 'description'])\n        return\n\n    # 3. Aggregate span counts\n    df_trace.set_index('datetime', inplace=True)\n    span_counts_1min = df_trace.groupby('cmdb_id').resample(COUNT_DROP_AGG_FREQ_INITIAL).size().unstack(level=0).fillna(0)\n\n    # 4. Calculate rolling sums and detect drops\n    span_counts_3min = span_counts_1min.rolling(window=COUNT_DROP_WINDOW_MINUTES, min_periods=1).sum()\n    previous_span_counts_3min = span_counts_3min.shift(COUNT_DROP_WINDOW_MINUTES)\n\n    # Condition: current < previous * (1 - ratio) AND previous > 0\n    is_drop = (span_counts_3min < previous_span_counts_3min * (1 - COUNT_DROP_THRESHOLD_RATIO)) & (previous_span_counts_3min > 0)\n\n    # 5. Identify anomaly events\n    simultaneous_drops = is_drop.sum(axis=1)\n    anomaly_timestamps = simultaneous_drops[simultaneous_drops >= COUNT_DROP_MIN_COMPONENTS].index\n\n    # 6. Format and report results\n    anomaly_events = []\n    final_anomaly_timestamps = anomaly_timestamps[(anomaly_timestamps >= start_time) & (anomaly_timestamps < end_time)]\n\n    for ts in final_anomaly_timestamps:\n        dropping_components = is_drop.loc[ts][is_drop.loc[ts]].index\n        \n        for comp in dropping_components:\n            current_count = span_counts_3min.loc[ts, comp]\n            previous_count = previous_span_counts_3min.loc[ts, comp]\n            \n            if pd.notna(current_count) and pd.notna(previous_count):\n                description = (f\"Span count dropped by more than {COUNT_DROP_THRESHOLD_RATIO*100}%. \"\n                               f\"Current 3-min count: {int(current_count)}, \"\n                               f\"Previous 3-min count: {int(previous_count)}.\")\n                \n                anomaly_event = {\n                    \"data_source\": \"Trace\",\n                    \"timestamp\": ts.strftime('%Y-%m-%d %H:%M:%S'),\n                    \"cmdb_id\": comp,\n                    \"description\": description\n                }\n                anomaly_events.append(anomaly_event)\n\n    # 7. Final Output\n    print(f\"anomaly_events = {str(anomaly_events)}\")\n\n    output_df = pd.DataFrame(anomaly_events)\n    if output_df.empty:\n        output_df = pd.DataFrame(columns=['data_source', 'timestamp', 'cmdb_id', 'description'])\n    \n    output_df.to_csv(\"anomaly_events.csv\", index=False)\n\nif __name__ == \"__main__\":\n    detect_trace_anomalies()\n",
    "timestamp": "2025-07-25 23:53:41"
  },
  {
    "language": "python",
    "code": "import pandas as pd\nimport pytz\nimport datetime\nimport os\nimport numpy as np\nimport csv\nimport math\n\ndef is_stable(counts, threshold):\n    \"\"\"\n    Checks if a list of peer counts is stable based on the coefficient of variation.\n    \"\"\"\n    counts = np.array(counts, dtype=float)\n    if len(counts) < 1:\n        return True\n    \n    mean = np.mean(counts)\n    std_dev = np.std(counts)\n    \n    if mean == 0:\n        return True\n        \n    cv = std_dev / mean\n    return cv <= threshold\n\ndef find_log_anomalies_refined():\n    \"\"\"\n    Detects network anomalies based on significant log count drops, following a structured workflow.\n    \"\"\"\n    # --- Core Parameters (stricter thresholds to reduce noise) ---\n    WINDOW_MINUTES = 5\n    THRESHOLD_RATIO = 0.6\n    STABILITY_THRESHOLD = 0.2\n    MIN_AVG_COUNT = 500\n    HISTORICAL_DECREASE_RATIO = 0.6\n\n    # --- Time and Path Definitions ---\n    target_date = '2021_03_04'\n    log_file_path = f'./dataset/Bank/telemetry/{target_date}/log/log_service.csv'\n    start_time_str = '2021-03-04 17:55:00' \n    end_time_str = '2021-03-04 18:30:00'\n    \n    anomaly_events = []\n    tz = pytz.timezone('Asia/Shanghai')\n\n    # --- 1. Data Preparation ---\n    if not os.path.exists(log_file_path):\n        return []\n\n    df = pd.read_csv(log_file_path)\n    df['datetime'] = pd.to_datetime(df['timestamp'], unit='s', utc=True).dt.tz_convert(tz)\n\n    start_time = tz.localize(datetime.datetime.strptime(start_time_str, '%Y-%m-%d %H:%M:%S'))\n    end_time = tz.localize(datetime.datetime.strptime(end_time_str, '%Y-%m-%d %H:%M:%S'))\n    \n    df = df[(df['datetime'] >= start_time) & (df['datetime'] < end_time)]\n\n    df['component_type'] = df['cmdb_id'].str.extract(r'([a-zA-Z]+)')[0]\n    target_types = ['Tomcat', 'apache']\n    df = df[df['component_type'].isin(target_types)]\n\n    if df.empty:\n        return []\n\n    # --- 2. Create Time-Windowed Count Table ---\n    time_bins = pd.date_range(start=start_time, end=end_time, freq=f'{WINDOW_MINUTES}min', tz=tz)\n    df['time_window'] = pd.cut(df['datetime'], bins=time_bins, right=False, labels=time_bins[:-1])\n    df.dropna(subset=['time_window'], inplace=True)\n\n    # Create a DataFrame with time_window as index and cmdb_id as columns\n    counts_df = df.groupby(['time_window', 'cmdb_id'], observed=True).size().unstack(fill_value=0)\n    \n    # --- 3. Iterate Through Windows and Apply Rules ---\n    analysis_start_time = tz.localize(datetime.datetime.strptime('2021-03-04 18:00:00', '%Y-%m-%d %H:%M:%S'))\n\n    for window_start in counts_df.index:\n        if window_start < analysis_start_time:\n            continue\n\n        previous_window_start = window_start - pd.Timedelta(minutes=WINDOW_MINUTES)\n        \n        for comp_type in target_types:\n            all_components_of_type = [c for c in counts_df.columns if c.startswith(comp_type)]\n            \n            if len(all_components_of_type) < 2:\n                continue\n\n            current_counts_list = [counts_df.loc[window_start, comp] for comp in all_components_of_type if comp in counts_df.columns]\n            if not current_counts_list:\n                continue\n            \n            avg_count = np.mean(current_counts_list)\n\n            if avg_count < MIN_AVG_COUNT:\n                continue\n\n            for cmdb_id in all_components_of_type:\n                if cmdb_id not in counts_df.columns:\n                    continue\n                \n                count = counts_df.loc[window_start, cmdb_id]\n\n                if count > 0 and count < (avg_count * THRESHOLD_RATIO):\n                    peer_counts = [counts_df.loc[window_start, p] for p in all_components_of_type if p != cmdb_id and p in counts_df.columns]\n                    \n                    if is_stable(peer_counts, STABILITY_THRESHOLD):\n                        if previous_window_start in counts_df.index:\n                            historical_count = counts_df.loc[previous_window_start].get(cmdb_id, 0)\n                            \n                            if count < (historical_count * HISTORICAL_DECREASE_RATIO):\n                                peer_avg = np.mean(peer_counts) if peer_counts else 0\n                                if count > 1 and peer_avg > 1:\n                                    delta = math.log(peer_avg) / math.log(count)\n                                else:\n                                    delta = float('inf')\n\n                                description = (f\"Log count for '{cmdb_id}' dropped to {count}, \"\n                                               f\"which is significantly below the peer average of {peer_avg:.0f} \"\n                                               f\"and its own previous count of {historical_count}. Delta: {delta:.2f}\")\n\n                                anomaly_events.append({\n                                    'data_source': 'Log',\n                                    'timestamp': window_start.strftime('%Y-%m-%d %H:%M:%S'),\n                                    'cmdb_id': cmdb_id,\n                                    'description': description\n                                })\n    return anomaly_events\n\nif __name__ == '__main__':\n    anomaly_events = find_log_anomalies_refined()\n\n    print(\"anomaly_events = [\")\n    if anomaly_events:\n        for i, event in enumerate(anomaly_events):\n            print(f\"    {event}\" + (\",\" if i < len(anomaly_events) - 1 else \"\"))\n    print(\"]\")\n\n    if anomaly_events:\n        output_file_path = 'anomaly_events.csv'\n        try:\n            with open(output_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n                fieldnames = ['data_source', 'timestamp', 'cmdb_id', 'description']\n                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n                writer.writeheader()\n                writer.writerows(anomaly_events)\n        except IOError as e:\n            pass\n",
    "timestamp": "2025-07-25 23:56:29"
  }
]